<html>
<link type="text/css" rel="stylesheet" id="dark-mode-custom-link">
<link type="text/css" rel="stylesheet" id="dark-mode-general-link">
<style lang="en" type="text/css" id="dark-mode-custom-style"></style>
<style lang="en" type="text/css" id="dark-mode-native-style"></style>
<style lang="en" type="text/css" id="dark-mode-native-sheet"></style>

<head></head>

<body>
    <div role="dialog" aria-modal="true" tabindex="-1" aria-labelledby="launch-modal-title--561"
        class="mobile-curriculum-item--dialog--2wWVO mobile-curriculum-item--dialog-closable--2isga">
        <h2 id="launch-modal-title--561" class="ud-sr-only">138. Machine Learning Model Evaluation</h2>
        <section class="lecture-view--container--pL22J"
            aria-label="Section 9: Scikit-learn: Creating Machine Learning Models, Lecture 138: Machine Learning Model Evaluation">
            <div class="text-viewer--container--18Ayx">
                <div class="text-viewer--scroll-container--1iy0Z">
                    <div class="text-viewer--content--3hoqQ">
                        <div class="ud-heading-xxl text-viewer--main-heading--ZbxZA">Machine Learning Model Evaluation
                        </div>
                        <div class="article-asset--container--3djM8">
                            <div data-purpose="safely-set-inner-html:rich-text-viewer:html"
                                class="article-asset--content--1dAQ9 rt-scaffolding">
                                <p>Evaluating the results of a machine learning model is as important as building one.
                                </p>
                                <p>But just like how different problems have different machine learning models,
                                    different
                                    machine learning models have different evaluation metrics.</p>
                                <p>Below are some of the most important evaluation metrics you'll want to look into for
                                    classification and regression models.</p>
                                <p><strong>Classification Model Evaluation Metrics/Techniques</strong></p>
                                <ul>
                                    <li>
                                        <p><strong>Accuracy</strong> - The accuracy of the model in decimal form.
                                            Perfect
                                            accuracy is equal to 1.0.</p>
                                    </li>
                                    <li>
                                        <p><a target="_blank" rel="noopener noreferrer"
                                                href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score"><strong>Precision</strong></a>
                                            - Indicates the proportion of positive identifications (model predicted
                                            class 1)
                                            which were actually correct. A model which produces no false positives has a
                                            precision of 1.0.</p>
                                    </li>
                                    <li>
                                        <p><a target="_blank" rel="noopener noreferrer"
                                                href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score"><strong>Recall</strong></a>
                                            - Indicates the proportion of actual positives which were correctly
                                            classified.
                                            A model which produces no false negatives has a recall of 1.0.</p>
                                    </li>
                                    <li>
                                        <p><a target="_blank" rel="noopener noreferrer"
                                                href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score"><strong>F1
                                                    score</strong></a> - A combination of precision and recall. A
                                            perfect
                                            model achieves an F1 score of 1.0.</p>
                                    </li>
                                    <li>
                                        <p><a target="_blank" rel="noopener noreferrer"
                                                href="https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/"><strong>Confusion
                                                    matrix</strong></a><strong> </strong>- Compares the predicted values
                                            with the true values in a tabular way, if 100% correct, all values in the
                                            matrix
                                            will be top left to bottom right (diagonal line).</p>
                                    </li>
                                    <li>
                                        <p><a target="_blank" rel="noopener noreferrer"
                                                href="https://scikit-learn.org/stable/modules/cross_validation.html"><strong>Cross-validation</strong></a>
                                            - Splits your dataset into multiple parts and train and tests your model on
                                            each
                                            part then evaluates performance as an average. </p>
                                    </li>
                                    <li>
                                        <p><a target="_blank" rel="noopener noreferrer"
                                                href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"><strong>Classification
                                                    report</strong></a><strong> </strong>- Sklearn has a built-in
                                            function
                                            called <code>classification_report()</code> which returns some of the main
                                            classification metrics such as precision, recall and f1-score.</p>
                                    </li>
                                    <li>
                                        <p><a target="_blank" rel="noopener noreferrer"
                                                href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_score.html"><strong>ROC
                                                    Curve</strong></a> - Also known as <a target="_blank"
                                                rel="noopener noreferrer"
                                                href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">receiver
                                                operating characteristic</a> is a plot of true positive rate versus
                                            false-positive rate.</p>
                                    </li>
                                    <li>
                                        <p><a target="_blank" rel="noopener noreferrer"
                                                href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"><strong>Area
                                                    Under Curve (AUC) Score</strong></a><strong> </strong>- The area
                                            underneath the ROC curve. A perfect model achieves an AUC&nbsp;score of 1.0.
                                        </p>
                                    </li>
                                </ul>
                                <p><strong>Which classification metric should you use?</strong></p>
                                <ul>
                                    <li>
                                        <p><strong>Accuracy</strong> is a good measure to start with if all classes are
                                            balanced (e.g. same amount of samples which are labelled with 0 or 1).</p>
                                    </li>
                                    <li>
                                        <p><strong>Precision</strong> and <strong>recall </strong>become more important
                                            when
                                            classes are imbalanced.</p>
                                    </li>
                                    <li>
                                        <p>If false-positive predictions are worse than false-negatives, aim for higher
                                            precision.</p>
                                    </li>
                                    <li>
                                        <p>If false-negative predictions are worse than false-positives, aim for higher
                                            recall.</p>
                                    </li>
                                    <li>
                                        <p><strong>F1-score</strong> is a combination of precision and recall.</p>
                                    </li>
                                    <li>
                                        <p>A confusion matrix is always a good way to visualize how a classification
                                            model
                                            is going.</p>
                                    </li>
                                </ul>
                                <p><strong>Regression Model Evaluation Metrics/Techniques</strong></p>
                                <ul>
                                    <li>
                                        <p><a target="_blank" rel="noopener noreferrer"
                                                href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html"><strong>R^2
                                                    (pronounced r-squared) or the coefficient of
                                                    determination</strong></a>
                                            - Compares your model's predictions to the mean of the targets. Values can
                                            range
                                            from negative infinity (a very poor model) to 1. For example, if all your
                                            model
                                            does is predict the mean of the targets, its R^2 value would be 0. And if
                                            your
                                            model perfectly predicts a range of numbers it's R^2 value would be 1. </p>
                                    </li>
                                    <li>
                                        <p><a target="_blank" rel="noopener noreferrer"
                                                href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html"><strong>Mean
                                                    absolute error (MAE)</strong></a> - The average of the absolute
                                            differences between predictions and actual values. It gives you an idea of
                                            how
                                            wrong your predictions were.</p>
                                    </li>
                                    <li>
                                        <p><a target="_blank" rel="noopener noreferrer"
                                                href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html"><strong>Mean
                                                    squared error (MSE)</strong></a> - The average squared differences
                                            between predictions and actual values. Squaring the errors removes negative
                                            errors. It also amplifies outliers (samples which have larger errors).</p>
                                    </li>
                                </ul>
                                <p><strong>Which regression metric should you use?</strong></p>
                                <ul>
                                    <li>
                                        <p><strong>R2</strong> is similar to accuracy. It gives you a quick indication
                                            of
                                            how well your model might be doing. Generally, the closer your
                                            <strong>R2</strong> value is to 1.0, the better the model. But it doesn't
                                            really
                                            tell exactly how wrong your model is in terms of how far off each prediction
                                            is.
                                        </p>
                                    </li>
                                    <li>
                                        <p><strong>MAE</strong> gives a better indication of how far off each of your
                                            model's predictions are on average.</p>
                                    </li>
                                    <li>
                                        <p>As for <strong>MAE</strong> or <strong>MSE</strong>, because of the way MSE
                                            is
                                            calculated, squaring the differences between predicted values and actual
                                            values,
                                            it amplifies larger differences. Let's say we're predicting the value of
                                            houses
                                            (which we are).</p>
                                        <ul>
                                            <li>
                                                <p>Pay more attention to MAE: When being $10,000 off is
                                                    <strong><em>twice</em></strong> as bad as being $5,000 off.
                                                </p>
                                            </li>
                                            <li>
                                                <p>Pay more attention to MSE: When being $10,000 off is <strong><em>more
                                                            than twice</em></strong> as bad as being $5,000 off.</p>
                                            </li>
                                        </ul>
                                    </li>
                                </ul>
                                <p>For more resources on evaluating a machine learning model, be sure to check out the
                                    following resources:</p>
                                <ul>
                                    <li>
                                        <p><a target="_blank" rel="noopener noreferrer"
                                                href="https://scikit-learn.org/stable/modules/model_evaluation.html">Scikit-Learn
                                                documentation for metrics and scoring (quantifying the quality of
                                                predictions)</a></p>
                                    </li>
                                    <li>
                                        <p><a target="_blank" rel="noopener noreferrer"
                                                href="https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c">Beyond
                                                Accuracy: Precision and Recall by Will Koehrsen</a></p>
                                    </li>
                                    <li>
                                        <p><a target="_blank" rel="noopener noreferrer"
                                                href="https://stackoverflow.com/a/37861832">Stack Overflow answer
                                                describing
                                                MSE (mean squared error) and RSME&nbsp;(root mean squared error)</a></p>
                                    </li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
                <footer
                    class="ct-dashboard-separator curriculum-item-footer--flex-align-center--3ja06 curriculum-item-footer--footer--Rin15">
                    <div class="curriculum-item-footer--flex-align-center--3ja06 curriculum-item-footer--right--2UpXX">
                        <div class="curriculum-item-footer--flex-align-center--3ja06">
                            <div class="popper-module--popper--2BpLn">

                                <div id="popper-content--564" aria-labelledby="control-bar-dropdown-trigger--562"
                                    class="popper-module--popper-content--3cLBV"
                                    style="bottom: 100%; right: 0px; margin-bottom: 0.8rem;">
                                    <div class="popper-module--animation-wrapper--2ogNt">
                                        <div id="control-bar-dropdown-menu--563" style="max-height: 28rem;"
                                            class="control-bar-dropdown--menu--2bFbL">

                                        </div>
                                    </div>
                                </div>
                            </div>


                        </div>
                    </div>
                </footer>
            </div>
        </section>
    </div>
</body>

</html>